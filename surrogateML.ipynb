{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51dce515",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a820cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb_client_3 import InfluxDBClient3\n",
    "import pandas as pd  \n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e10c6",
   "metadata": {},
   "source": [
    "In this part we perform the Data Backup from influxDB. As a result, sparced dataset with raw data stored in folder 'influxDB Backup' in root folder of this repository. File format: parquet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69deb9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting export of 'saw'...\n",
      "Fetching rows 0 to 1000000...\n",
      "CRITICAL ERROR at offset 0: Error while executing query: Flight returned unavailable error, with message: failed to connect to all addresses; last error: UNKNOWN: ipv4:10.65.1.77:10102: tcp handshaker shutdown. gRPC client debug context: UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:10.65.1.77:10102: tcp handshaker shutdown\", grpc_status:14, created_time:\"2026-01-04T13:03:48.451546+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "No data was retrieved.\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 1_000_000\n",
    "\n",
    "client = InfluxDBClient3(\n",
    "    token=\"apiv3_9EL5Mq-kpLIFd1y3dc9ow0JWWnGPMLM9h6lcFQxQt02FZ-SZis7eJYmj27-E8EKlA_3MTUGa_QBd0holy94oEQ\",\n",
    "    host=\"http://10.65.1.77:10102\",\n",
    "    database=\"spp1-main\"\n",
    ")\n",
    "\n",
    "TABLE_NAME = \"saw\"\n",
    "OUTPUT_FILE = \"influxDB Backup/saw.backup.parquet\"\n",
    "\n",
    "offset = 0\n",
    "all_frames = [] # List to store each chunk in memory\n",
    "\n",
    "print(f\"Starting export of '{TABLE_NAME}'...\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Fetching rows {offset} to {offset + LIMIT}...\", end=\"\\r\")\n",
    "\n",
    "    # Order by time is critical to ensure pages don't shuffle or overlap\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM \"{TABLE_NAME}\" \n",
    "        ORDER BY time ASC\n",
    "        LIMIT {LIMIT} OFFSET {offset}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Fetch the chunk\n",
    "        df_chunk = client.query(query=query, mode=\"pandas\")\n",
    "\n",
    "        # If chunk is empty, we are done\n",
    "        if df_chunk.empty:\n",
    "            print(f\"\\nNo more data found after offset {offset}.\")\n",
    "            break\n",
    "        \n",
    "        # Append to our in-memory list\n",
    "        all_frames.append(df_chunk)\n",
    "\n",
    "        # Check if this was the last page (less rows than limit)\n",
    "        if len(df_chunk) < LIMIT:\n",
    "            print(f\"\\nReached end of dataset at offset {offset + len(df_chunk)}.\")\n",
    "            break\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        offset += LIMIT\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCRITICAL ERROR at offset {offset}: {e}\")\n",
    "        # Depending on importance, you might want to 'break' or 'raise' here\n",
    "        break\n",
    "\n",
    "# --- Final Consolidation ---\n",
    "if all_frames:\n",
    "    print(\"Concatenating data...\")\n",
    "    final_df = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    print(f\"Saving {len(final_df)} rows to {OUTPUT_FILE}...\")\n",
    "    final_df.to_parquet(OUTPUT_FILE, index=False)\n",
    "    print(\"Export successful.\")\n",
    "else:\n",
    "    print(\"No data was retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f7b03",
   "metadata": {},
   "source": [
    "Parquet to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f675739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /Users/vvsmbk/Desktop/all/3 semester classes/Smart Production and Prototyping/Practical Part/gitProject/projectRaphael/influxDB Backup/saw.backup.csv\n",
      "Exists: True Size: 7143765\n"
     ]
    }
   ],
   "source": [
    "in_path = Path(\"influxDB Backup/saw.backup.parquet\")\n",
    "out_path = in_path.with_suffix(\".csv\")\n",
    "\n",
    "df = pq.read_table(in_path).to_pandas()\n",
    "df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved to:\", out_path.resolve())\n",
    "print(\"Exists:\", out_path.exists(), \"Size:\", out_path.stat().st_size if out_path.exists() else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7ee36",
   "metadata": {},
   "source": [
    "\"Compression\" of the raw csv file: after that step, all the waterflow presented data reorganized in wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99400e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"influxDB Backup/saw.backup.csv\")\n",
    "\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"time_1s\"] = df[\"time\"].dt.floor(\"s\")   # .dt.round(\"s\") for rounding\n",
    "\n",
    "value_cols = [c for c in df.columns if c not in (\"time\", \"time_1s\")]\n",
    "\n",
    "out = (\n",
    "    df.sort_values(\"time\")\n",
    "      .groupby(\"time_1s\")[value_cols]\n",
    "      .last()                      # last not-NaN for each column\n",
    "      .reset_index()\n",
    "      .rename(columns={\"time_1s\": \"time\"})\n",
    ")\n",
    "\n",
    "out.to_csv(\"influxDB Backup/saw_wide.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
